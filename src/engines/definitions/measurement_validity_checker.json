{
  "engine_key": "measurement_validity_checker",
  "engine_name": "Measurement Validity Checker",
  "description": "Assesses whether measurements validly capture their intended constructs. Evaluates operationalization quality, proxy measures, measurement error, and impact on conclusions. Essential for ensuring findings measure what they claim to measure.",
  "version": 1,
  "category": "methodology",
  "kind": "synthesis",
  "reasoning_domain": "methodology",
  "researcher_question": "Do the measurements actually capture what they claim to measure?",
  "canonical_schema": {
    "constructs_measured": [
      {
        "construct_id": "string - CON1, CON2, etc.",
        "construct_name": "string - what concept is being measured",
        "theoretical_definition": "string - what the concept means",
        "operationalization": "string - how it's actually measured",
        "measurement_type": "direct | proxy | self_report | behavioral | composite"
      }
    ],
    "validity_assessments": [
      {
        "assessment_id": "string",
        "construct_id": "string",
        "overall_validity": "high | moderate | low | very_low",
        "face_validity": {
          "rating": "high | moderate | low",
          "rationale": "string - does it look like it measures the concept?"
        },
        "construct_validity": {
          "rating": "high | moderate | low | unknown",
          "rationale": "string - does it really capture the concept?",
          "convergent_evidence": "string - does it correlate with similar measures?",
          "discriminant_evidence": "string - does it differ from dissimilar measures?"
        },
        "content_validity": {
          "rating": "high | moderate | low",
          "rationale": "string - does it cover all aspects of the concept?",
          "aspects_covered": [
            "string"
          ],
          "aspects_missing": [
            "string"
          ]
        },
        "reliability": "high | moderate | low | unknown"
      }
    ],
    "measurement_problems": [
      {
        "problem_id": "string",
        "construct_id": "string",
        "problem_type": "construct_mismatch | proxy_weakness | measurement_error | social_desirability | reactivity | narrow_operationalization",
        "description": "string - what the problem is",
        "severity": "severe | significant | moderate | minor",
        "impact_on_conclusions": "string - how this affects findings"
      }
    ],
    "proxy_measure_analysis": [
      {
        "proxy_id": "string",
        "construct_id": "string",
        "intended_construct": "string - what we want to measure",
        "actual_measure": "string - what we're actually measuring",
        "gap_description": "string - difference between intended and actual",
        "why_used": "string - why proxy is used instead of direct measure",
        "validity_of_proxy": "good | acceptable | questionable | poor"
      }
    ],
    "operationalization_critique": [
      {
        "construct_id": "string",
        "critique": "string - what's wrong with operationalization",
        "alternative_operationalization": "string - better way to measure",
        "why_better": "string"
      }
    ],
    "conclusion_validity_impact": [
      {
        "conclusion": "string - finding or claim",
        "measurement_issues": [
          "string - problem_ids affecting this"
        ],
        "conclusion_validity": "robust | weakened | undermined | invalidated",
        "caveat_needed": "string - how to qualify the conclusion"
      }
    ],
    "meta": {
      "constructs_analyzed": "number",
      "high_validity_measures": "number",
      "problematic_measures": "number",
      "most_common_problem": "string",
      "overall_measurement_quality": "high | medium | low"
    }
  },
  "extraction_focus": [
    "constructs_measured",
    "operationalizations",
    "validity_assessment",
    "measurement_problems",
    "conclusion_impact"
  ],
  "primary_output_modes": [
    "structured_text_report",
    "table"
  ],
  "paradigm_keys": [],
  "source_file": "analyzer/src/engines/measurement_validity_checker.py",
  "stage_context": {
    "framework_key": null,
    "additional_frameworks": [],
    "extraction": {
      "analysis_type": "measurement validity checker",
      "analysis_type_plural": "measurement validity checkers",
      "core_question": "Do the measurements actually capture what they claim to measure?",
      "extraction_steps": [
        "IDENTIFY CONSTRUCTS",
        "ASSESS OPERATIONALIZATION QUALITY",
        "ANALYZE PROXY MEASURES",
        "IDENTIFY MEASUREMENT PROBLEMS",
        "ASSESS CONCLUSION IMPACT"
      ],
      "key_fields": {},
      "id_field": "measurement_id",
      "key_relationships": [
        "relates_to"
      ],
      "special_instructions": null
    },
    "curation": {
      "item_type": "measurement validity checker",
      "item_type_plural": "measurement validity checkers",
      "consolidation_rules": [],
      "cross_doc_patterns": [
        "shared_items",
        "contested_items",
        "response_network"
      ],
      "synthesis_outputs": [
        "consolidated_item_list",
        "relationship_map",
        "cross_document_dynamics"
      ],
      "special_instructions": null
    },
    "concretization": {
      "id_examples": [],
      "naming_guidance": "",
      "recommended_table_types": [],
      "recommended_visual_patterns": []
    },
    "audience_vocabulary": {
      "researcher": {},
      "analyst": {},
      "executive": {},
      "activist": {}
    },
    "skip_concretization": false
  }
}